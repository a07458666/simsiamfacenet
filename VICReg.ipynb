{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "hungry-posting",
   "metadata": {},
   "source": [
    "# VICReg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "statutory-batch",
   "metadata": {},
   "source": [
    "<img src=\"img/VICReg.png\" width=\"50%\">  \n",
    "https://github.com/vturrisi/solo-learn/blob/da26b4dcbd69dab45593ceb031f04568f7659667/solo/losses/vicreg.py#L82"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ignored-belly",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "\n",
    "import os\n",
    "import PIL.Image as Image\n",
    "import numpy as np\n",
    "from os import listdir\n",
    "from os import walk\n",
    "import glob\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "\n",
    "import simsiam.loader\n",
    "\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "from PIL import Image, ImageFilter\n",
    "from torch.utils.data import DataLoader\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import torchvision.models as models\n",
    "from facenet_pytorch import InceptionResnetV1\n",
    "from torch import nn\n",
    "from loss_functions.vicreg import vicreg_loss_func\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bridal-retention",
   "metadata": {},
   "source": [
    "## GPU Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "possible-boost",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch version:1.8.0\n",
      "CUDA is not available.\n"
     ]
    }
   ],
   "source": [
    "DATA_PATH_TRAIN = '../../dataset/face_labeled_data/train'\n",
    "DATA_PATH_VAL = '../../dataset/face_labeled_data/val'\n",
    "batch_size = 32\n",
    "# learning rate\n",
    "init_learning_rate = 0.05 * batch_size / 256\n",
    "momentum_val = 0.9\n",
    "weight_decay_val = 1e-4\n",
    "\n",
    "sim_weight=25.\n",
    "var_weight=25.\n",
    "cov_weight=1.\n",
    "\n",
    "print('torch version:' + torch.__version__)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print('Available GPUs: ', end='')\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(torch.cuda.get_device_name(i), end=' ')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print('CUDA is not available.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "literary-spyware",
   "metadata": {},
   "source": [
    "## define dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "coupled-fabric",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = (224, 224)\n",
    "\n",
    "class FaceImages(Dataset):\n",
    "    \n",
    "    def __init__(self, img_dir, transform, specific = '**'):\n",
    "        self.img_dir = img_dir\n",
    "        self.img_path_list = glob.glob(os.path.join(img_dir, specific + '/*.jpg'))\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.img_path_list)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.img_path_list[idx]\n",
    "        img = FaceImages.read_image(img_path)\n",
    "        id = img_path.split('/')[5]\n",
    "        # print(\"ID : \", id)\n",
    "        return self.transform(img), self.transform(img), id\n",
    "    \n",
    "    @staticmethod\n",
    "    def read_image(img_path):\n",
    "        #return cv2.imread(img_path)\n",
    "        return Image.open(img_path, mode='r').convert('RGB')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "direct-justice",
   "metadata": {},
   "source": [
    "## define data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "young-detector",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "                          \n",
    "def get_aug_trnsform(s=1.0):\n",
    "    color_jitter = transforms.ColorJitter(0.8*s, 0.8*s, 0.8*s, 0.1)\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224, scale=(0.2, 1.)),\n",
    "        transforms.RandomApply([\n",
    "            transforms.ColorJitter(0.4, 0.4, 0.4, 0.1)  # not strengthened\n",
    "        ], p=0.8),\n",
    "        transforms.RandomGrayscale(p=0.2),\n",
    "        transforms.RandomApply([simsiam.loader.GaussianBlur([.1, 2.])], p=0.5),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        normalize\n",
    "    ])\n",
    "    \n",
    "    return transform\n",
    "\n",
    "def ge_eval_trnsform(s=1.0):\n",
    "    color_jitter = transforms.ColorJitter(0.8*s, 0.8*s, 0.8*s, 0.1)\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        normalize\n",
    "    ])\n",
    "    \n",
    "    return transform\n",
    "\n",
    "aug = get_aug_trnsform(0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "african-latex",
   "metadata": {},
   "source": [
    "## define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "sudden-productivity",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make embedding to length=1\n",
    "class L2_norm(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(L2_norm, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.normalize(x, p=2, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "alone-train",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "  (1): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (2): ReLU()\n",
       "  (3): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "  (4): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (5): ReLU()\n",
       "  (6): Linear(in_features=2048, out_features=512, bias=True)\n",
       "  (7): L2_norm()\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# encoder\n",
    "encoder = InceptionResnetV1()\n",
    "encoder.to(device)\n",
    "# projector\n",
    "projector = nn.Sequential(\n",
    "    nn.Linear(512, 2048), nn.BatchNorm1d(2048), nn.ReLU(),\n",
    "    nn.Linear(2048, 2048), nn.BatchNorm1d(2048), nn.ReLU(),\n",
    "    nn.Linear(2048, 512), L2_norm()\n",
    ") \n",
    "projector.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "published-computer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer\n",
    "from torch import optim\n",
    "optim_params_encoder = [{'params': encoder.parameters(), 'fix_lr': False}]\n",
    "optim_params_projector = [{'params': projector.parameters(), 'fix_lr': False}]\n",
    "\n",
    "encoder_opt = optim.SGD(optim_params_encoder, lr=init_learning_rate, momentum = momentum_val, weight_decay = weight_decay_val)\n",
    "projector_opt = optim.SGD(optim_params_projector, lr=init_learning_rate, momentum = momentum_val, weight_decay = weight_decay_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "subsequent-bradford",
   "metadata": {},
   "source": [
    "### save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "blind-america",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(state, is_best, filename='checkpoint.pth.tar'):\n",
    "    torch.save(state, filename)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filename, 'model_best.pth.tar')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mysterious-camera",
   "metadata": {},
   "source": [
    "### 動態lr "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "renewable-handling",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_learning_rate(optimizer, init_lr, epoch, epochs):\n",
    "    \"\"\"Decay the learning rate based on schedule\"\"\"\n",
    "    cur_lr = init_lr * 0.5 * (1. + math.cos(math.pi * epoch / epochs))\n",
    "    for param_group in optimizer.param_groups:\n",
    "        if 'fix_lr' in param_group and param_group['fix_lr']:\n",
    "            param_group['lr'] = init_lr\n",
    "        else:\n",
    "            param_group['lr'] = cur_lr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "marine-debate",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "collect-command",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pass_epoch(loader, mode = 'Train'):\n",
    "    loss = 0\n",
    "    loss_sim = 0\n",
    "    loss_var = 0\n",
    "    loss_cov = 0\n",
    "    if (mode == 'Train'):\n",
    "        encoder.train()\n",
    "        projector.train()\n",
    "    else:\n",
    "        encoder.eval()\n",
    "        projector.eval()\n",
    "    for i_batch, image_batch in tqdm(enumerate(loader)):\n",
    "        x1, x2 = image_batch[0].to(device), image_batch[1].to(device)\n",
    "        # forward\n",
    "        y1, y2 = encoder(x1), encoder(x2)\n",
    "        z1, z2 = projector(y1), projector(y2)\n",
    "\n",
    "        # compute loos\n",
    "        loss_batch, loss_batch_sim, loss_batch_var, loss_batch_cov = vicreg_loss_func(z1, z2, sim_loss_weight=sim_weight, var_loss_weight=var_weight, cov_loss_weight=cov_weight) # loss\n",
    "        loss += loss_batch\n",
    "        loss_sim += loss_batch_sim\n",
    "        loss_var += loss_batch_var\n",
    "        loss_cov += loss_batch_cov\n",
    "        if mode == 'Train':\n",
    "            # update\n",
    "            encoder_opt.zero_grad()\n",
    "            projector_opt.zero_grad()\n",
    "            loss_batch.backward()\n",
    "            encoder_opt.step()\n",
    "            projector_opt.step()        \n",
    "    return loss / (i_batch + 1), loss_sim / (i_batch + 1), loss_var / (i_batch + 1), loss_cov / (i_batch + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "junior-video",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-512d66f13ef9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0madjust_learning_rate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprojector_opt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit_learning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loss_sim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loss_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loss_cov\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpass_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0mval_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loss_sim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loss_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loss_cov\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpass_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Eval'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-69b4a59d721f>\u001b[0m in \u001b[0;36mpass_epoch\u001b[0;34m(loader, mode)\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mprojector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_batch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0;31m# forward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0my1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0;31m# This function throws if there's a driver initialization error, no GPUs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0;31m# are found or any other error occurs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m         \u001b[0;31m# Some of the queued calls may reentrantly call _lazy_init();\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m         \u001b[0;31m# we need to just return without initializing in that case.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "from tqdm import tqdm \n",
    "\n",
    "dataset_train = FaceImages(DATA_PATH_TRAIN, aug)\n",
    "dataloader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=True)\n",
    "dataset_val = FaceImages(DATA_PATH_VAL, aug)\n",
    "dataloader_val = DataLoader(dataset_val, batch_size=int(batch_size), shuffle=True)\n",
    "\n",
    "epoch = 100\n",
    "\n",
    "loss_history_train = []\n",
    "loss_history_train_sim = []\n",
    "loss_history_train_var = []\n",
    "loss_history_train_cov = []\n",
    "loss_history_val = []\n",
    "loss_history_val_sim = []\n",
    "loss_history_val_var = []\n",
    "loss_history_val_cov = []\n",
    "\n",
    "def update_loss_hist(train_list, val_list, name='result'):\n",
    "    clear_output(wait=True)\n",
    "    plt.plot(train_list)\n",
    "    plt.plot(val_list)\n",
    "    plt.title('Loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['train', 'val'], loc='center right')\n",
    "    plt.savefig(name + '.png')\n",
    "    plt.show()\n",
    "\n",
    "# train loop\n",
    "for i in range(epoch):\n",
    "    adjust_learning_rate(encoder_opt, init_learning_rate, i, epoch)\n",
    "    adjust_learning_rate(projector_opt, init_learning_rate, i, epoch)\n",
    "    \n",
    "    train_loss, train_loss_sim, train_loss_var, train_loss_cov = pass_epoch(dataloader_train, 'Train')\n",
    "    with torch.no_grad():\n",
    "        val_loss, val_loss_sim, val_loss_var, val_loss_cov = pass_epoch(dataloader_val, 'Eval')\n",
    "\n",
    "    \n",
    "    loss_history_train.append(train_loss)\n",
    "    loss_history_train_sim.append(train_loss_sim)\n",
    "    loss_history_train_var.append(train_loss_var)\n",
    "    loss_history_train_cov.append(train_loss_cov)\n",
    "\n",
    "    loss_history_val.append(val_loss)\n",
    "    loss_history_val_sim.append(val_loss_sim)\n",
    "    loss_history_val_var.append(val_loss_var)\n",
    "    loss_history_val_cov.append(val_loss_cov)\n",
    "    update_loss_hist(loss_history_train, loss_history_val, 'Loss')\n",
    "    update_loss_hist(loss_history_train_sim, loss_history_val_sim, 'Loss')\n",
    "    update_loss_hist(loss_history_train_var, loss_history_val_var, 'Loss')\n",
    "    update_loss_hist(loss_history_train_cov, loss_history_val_cov, 'Loss')\n",
    "\n",
    "    save_checkpoint({\n",
    "        'epoch': i + 1,\n",
    "        'arch': 'vicreg',\n",
    "        'sim_weight': sim_weight,\n",
    "        'var_weight': var_weight,\n",
    "        'cov_weight': cov_weight,\n",
    "        'state_dict': nn.Sequential(encoder, projector).state_dict(),\n",
    "        'optimizer' : nn.Sequential(encoder, projector).state_dict(),\n",
    "    }, is_best=False, filename='./model_vicreg_v2/checkpoint_{:04d}.pth.tar'.format(i + 1))\n",
    "torch.save(nn.Sequential(encoder, projector), './model_vicreg_v2/checkpoint.pth.tar')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hollywood-flavor",
   "metadata": {},
   "source": [
    "### collapse check(simularity matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "funny-topic",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = \"./model_vicreg_v2/checkpoint.pth.tar\"\n",
    "model = torch.load(MODEL_PATH).to(device).eval()\n",
    "\n",
    "dataset_eval = FaceImages(DATA_PATH_VAL, ge_eval_trnsform(0.5))\n",
    "dataloader_eval = DataLoader(dataset_eval, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "def collapseCheck(model, loader):\n",
    "    x, _, _ = next(iter(loader))\n",
    "    h = model(x.to(device))\n",
    "    h_norm = h / h.norm(dim=1)[:, None]\n",
    "    res = torch.mm(h_norm, h_norm.transpose(0,1))\n",
    "    print(res.cpu().detach().numpy())\n",
    "    \n",
    "collapseCheck(model, dataloader_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thousand-gauge",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
